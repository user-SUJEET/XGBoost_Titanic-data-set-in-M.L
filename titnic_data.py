# -*- coding: utf-8 -*-
"""titnic data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Miu1kugermUEpBA-HcnGynhbJjmHLKj2
"""

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

df=pd.read_csv("/content/Titanic-Dataset.csv")

df

df.info()

df.tail()

df.describe()

df.isnull().sum()

df.duplicated().sum()

df.shape

df.hist(figsize=(10,10))
plt.show()

df.boxplot()

df.isnull().sum()

df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

display(df.isnull().sum())

df.boxplot()

sns.heatmap(df.isnull(), cbar=False)
plt.show()

df.isnull()

df.shape

df.columns

df['SibSp'].mean()

df['SibSp'].median()

df['SibSp'].min()

df.tail()

plt.scatter(df['SibSp'], df['Age'])
plt.title("Scatter Plot")
plt.xlabel('SibSp')
plt.ylabel('Age')
plt.colorbar()
plt.show()

(df['SibSp'], df['Age'])
plt.bar(df['SibSp'], data['Age'])

plt.hist(df['Age'])
plt.title("Histogram")
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

df.columns

from sklearn.model_selection import train_test_split

X = df.drop('Survived', axis=1)
y = df['Survived']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


print(X.shape, X_train.shape, X_test.shape)

X.shape,X_train.shape,X_test.shape

y.shape,y_train.shape,y_test.shape

df.shape

from xgboost import XGBClassifier

xgb_model=XGBClassifier()

xgb_model





"""# Task
Train an XGBoost model on the Titanic dataset to predict survival. Handle categorical features by dropping 'Name', 'Ticket', and 'Cabin' columns and applying one-hot encoding to 'Sex' and 'Embarked'. Evaluate the model's performance.

## Handle categorical features

### Subtask:
Drop 'Name', 'Ticket', and 'Cabin' columns. Apply one-hot encoding to the 'Sex' and 'Embarked' columns.

**Reasoning**:
Drop the specified columns from the training and testing sets and apply one-hot encoding to the categorical features 'Sex' and 'Embarked'.
"""

X_train_processed = X_train.drop(['Name', 'Ticket', 'Cabin'], axis=1)
X_test_processed = X_test.drop(['Name', 'Ticket', 'Cabin'], axis=1)

X_train_processed = pd.get_dummies(X_train_processed, columns=['Sex', 'Embarked'], drop_first=True)
X_test_processed = pd.get_dummies(X_test_processed, columns=['Sex', 'Embarked'], drop_first=True)

display(X_train_processed.head())
display(X_train_processed.columns)
display(X_test_processed.head())
display(X_test_processed.columns)

"""## Train the model

### Subtask:
Train the XGBoost model with the preprocessed data.

**Reasoning**:
Train the XGBoost model using the preprocessed training data.
"""

xgb_model.fit(X_train_processed, y_train)

"""## Evaluate the model

### Subtask:
Evaluate the performance of the trained XGBoost model.

**Reasoning**:
Import the necessary evaluation metrics and make predictions on the test set.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_pred = xgb_model.predict(X_test_processed)

"""**Reasoning**:
Calculate and print the evaluation metrics for the model.


"""

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

"""## Summary:

### Data Analysis Key Findings

*   The categorical features 'Sex' and 'Embarked' were successfully one-hot encoded, while 'Name', 'Ticket', and 'Cabin' columns were dropped from both training and testing datasets.
*   An XGBoost model was trained using the preprocessed training data.
*   The model achieved an accuracy of approximately 0.7933 on the test set.
*   The precision of the model was around 0.7606, and the recall was approximately 0.7297.
*   The F1 score for the model was about 0.7448.

### Insights or Next Steps

*   Further hyperparameter tuning of the XGBoost model could potentially improve performance metrics like accuracy and F1 score.
*   Investigate feature importance from the trained model to understand which features contribute most to the survival prediction.

"""